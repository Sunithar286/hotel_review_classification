{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc92454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fc6c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cd6937c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nice hotel expensive parking got good deal sta...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok nothing special charge diamond member hilto...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nice rooms not 4* experience hotel monaco seat...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unique, great stay, wonderful time hotel monac...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great stay great stay, went seahawk game aweso...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20486</th>\n",
       "      <td>best kept secret 3rd time staying charm, not 5...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20487</th>\n",
       "      <td>great location price view hotel great quick pl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20488</th>\n",
       "      <td>ok just looks nice modern outside, desk staff ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20489</th>\n",
       "      <td>hotel theft ruined vacation hotel opened sept ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20490</th>\n",
       "      <td>people talking, ca n't believe excellent ratin...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20491 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review  Rating\n",
       "0      nice hotel expensive parking got good deal sta...       4\n",
       "1      ok nothing special charge diamond member hilto...       2\n",
       "2      nice rooms not 4* experience hotel monaco seat...       3\n",
       "3      unique, great stay, wonderful time hotel monac...       5\n",
       "4      great stay great stay, went seahawk game aweso...       5\n",
       "...                                                  ...     ...\n",
       "20486  best kept secret 3rd time staying charm, not 5...       5\n",
       "20487  great location price view hotel great quick pl...       4\n",
       "20488  ok just looks nice modern outside, desk staff ...       2\n",
       "20489  hotel theft ruined vacation hotel opened sept ...       1\n",
       "20490  people talking, ca n't believe excellent ratin...       2\n",
       "\n",
       "[20491 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(r'C:\\Users\\Suneetha\\python files\\PROJECT-2\\hotel_reviews.csv',encoding=\"Latin1\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65aa13cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "ps=PorterStemmer()\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae47a1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from spacy) (8.1.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from spacy) (1.9.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\suneetha\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000016434B611F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/spacy/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000016434B45790>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/spacy/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000016434B455E0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/spacy/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000016434B45370>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/spacy/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000016434B612E0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/spacy/\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda update spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2fd56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wasabi==0.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f9257",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "STOP_WORDS |= {\"nt\",\"hotel\",\"room\"}\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6a217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ceee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(text):\n",
    "    text=text.lower()\n",
    "   \n",
    "    text=re.sub(\"\\[.*?\\]\",\"\",text)\n",
    "    text=re.sub(\"[%s]\" % re.escape(string.punctuation),\"\",text)\n",
    "    text=re.sub(\"\\w*\\d\\w*\",\"\",text)\n",
    "    text=re.sub(\"\\n\",\"\",text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca55d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_1 = lambda x: text_clean(x)\n",
    "data[\"Cleaned_Reviews\"]=pd.DataFrame(data.Review.apply(cleaned_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reviews_1=data.copy()\n",
    "Reviews_1.drop([\"Review\",\"Rating\"],axis=1,inplace=True)\n",
    "Reviews_1[\"Cleaned_Reviews\"][6]\n",
    "Reviews_1.Cleaned_Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38551a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc842d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f06ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range  (0,len(Reviews_1)):\n",
    "    review=re.sub(\"[^a-zA-Z]\",\" \",Reviews_1[\"Cleaned_Reviews\"][i])\n",
    "    \n",
    "    review=review.split()\n",
    "    review=[lemmatizer.lemmatize(word) for word in review if not word in STOP_WORDS]\n",
    "    review=\" \".join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ccdcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6396b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Cleaned_Review_Lemmatized\"]=corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5845678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating positive wordcloud\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e97d978",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"positive-words.txt\",\"r\") as pos:\n",
    "    positive = pos.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4745107",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud1 = WordCloud(width = 3000, height = 2000, random_state=1, background_color='white', colormap='viridis', collocations=False, stopwords=positive).generate(str(corpus ))\n",
    "fig = plt.figure(figsize=(30,10), facecolor='white')\n",
    "plt.imshow(wordcloud1)\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da864295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating negative wordcloud\n",
    "\n",
    "with open(\"negative-words.txt\",\"r\") as neg:\n",
    "    negative = neg.read().split(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdba338",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_neg = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', colormap='Pastel1', collocations=False,stopwords=negative).generate(str(corpus))\n",
    "fig = plt.figure(figsize=(30,10), facecolor='white')\n",
    "plt.imshow(wordcloud_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d49cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2532b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polarity and subjectivity#\n",
    "\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "data[\"Polarity\"]=data[\"Cleaned_Reviews\"].apply(lambda x:TextBlob(x).sentiment.polarity)\n",
    "data[\"Subjectivity\"]=data[\"Cleaned_Reviews\"].apply(lambda x:TextBlob(x).sentiment.subjectivity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1c85a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing 5 reviews with highest polarity\n",
    "\n",
    "print(\"5 Random Reviews with Highest Polarity:\")\n",
    "for index,review in enumerate(data.iloc[data['Polarity'].sort_values(ascending=False)[:5].index]['Cleaned_Reviews']):\n",
    "    print('Review {}:\\n'.format(index+1),review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6288f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing 5 reviews with negative polarity\n",
    "\n",
    "print(\"5 Random Reviews with Lowest Polarity:\")\n",
    "for index,review in enumerate(data.iloc[data['Polarity'].sort_values(ascending=True)[:5].index]['Cleaned_Reviews']):\n",
    "  print('Review {}:\\n'.format(index+1),review)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42748e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency Distribution based on Polarity\n",
    "\n",
    "plt.figure(figsize=(30,20),edgecolor=\"black\")\n",
    "plt.margins(0.05)\n",
    "plt.xlabel(\"Polarity\",fontsize=50)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.ylabel(\"Frequency\",fontsize=50)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.hist(data[\"Polarity\"],bins=50)\n",
    "plt.title(\"Frequency Distribution based on Polarity\",fontsize=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd0d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pie plot of percentage of ratings\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.title('Percentage of Ratings', fontsize=20)\n",
    "data.Rating.value_counts().plot(kind='pie', labels=['Rating5', 'Rating4', 'Rating3', 'Rating2', 'Rating1'],\n",
    "                              wedgeprops=dict(width=.7), autopct='%1.0f%%', startangle= -20, \n",
    "                              textprops={'fontsize': 15})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03641b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment distribution based on ratings\n",
    "\n",
    "polarity_avg = data.groupby('Rating')['Polarity'].mean().plot(kind='bar', figsize=(30,20),color=\"green\",edgecolor=\"orange\")\n",
    "plt.xlabel('Rating', fontsize=45)\n",
    "plt.ylabel('Average Sentiment', fontsize=45)\n",
    "plt.xticks(fontsize=40)\n",
    "plt.yticks(fontsize=40)\n",
    "plt.title('Average Sentiment per Rating Distribution', fontsize=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20142f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting no of words in each review\n",
    "\n",
    "data[\"Word Count\"]=data[\"Cleaned_Review_Lemmatized\"].apply(lambda x:len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d03ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average no of words wrt ratings\n",
    "\n",
    "word_avg=data.groupby(\"Rating\")[\"Word Count\"].mean().plot(kind=\"bar\",figsize=(50,30),color=\"green\",edgecolor=\"orange\")\n",
    "plt.xlabel('Rating',fontsize=35)\n",
    "plt.ylabel(\"Average Count of Words\",fontsize=35)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.title(\"Average Count of Words wrt Ratings\",fontsize=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ee0473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting no of letters in each review\n",
    "\n",
    "data['review_len'] = data['Cleaned_Review_Lemmatized'].astype(str).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f146b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average no of letters wrt ratings\n",
    "\n",
    "letter_avg = data.groupby('Rating')['review_len'].mean().plot(kind='bar', figsize=(50,30),color=\"green\",edgecolor=\"orange\")\n",
    "plt.xlabel('Rating', fontsize=35)\n",
    "plt.ylabel('Count of Letters in Rating', fontsize=35)\n",
    "plt.xticks(fontsize=40)\n",
    "plt.yticks(fontsize=40)\n",
    "plt.title('Average Number of Letters wrt Rating ', fontsize=40)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341ca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting 100 most common words in our data\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "mostcommon = FreqDist(data[\"Cleaned_Review_Lemmatized\"]).most_common(100)\n",
    "wordcloud = WordCloud(width=1600, height=800, background_color='white').generate(str(corpus))\n",
    "fig = plt.figure(figsize=(30,10), facecolor='white')\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.title('Top 100 Most Common Words', fontsize=100)\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d9146",
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_positive_data=pd.DataFrame(data.groupby(\"Cleaned_Reviews\")[\"Polarity\"].mean().sort_values(ascending=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7732d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,30))\n",
    "plt.xlabel('Polarity',fontsize=35)\n",
    "plt.ylabel('Reviews',fontsize=35)\n",
    "plt.xticks(fontsize=35)\n",
    "plt.yticks(fontsize=35)\n",
    "plt.title('Polarity of Reviews',fontsize=50)\n",
    "polarity_graph=plt.barh(np.arange(len(polarity_positive_data.index)),polarity_positive_data['Polarity'],color='purple',)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af205a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorizing reviews in positive,negative and neutral#\n",
    "\n",
    "def sentiment(x):\n",
    "    if x<0:\n",
    "        return 'negative'\n",
    "    elif x==0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d44b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['polarity_score']=data['Polarity'].\\\n",
    "   map(lambda x: sentiment(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b6e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data[\"polarity_score\"],bins=10, color='orange', alpha=0.5, label='Value', edgecolor='black', linewidth=1)\n",
    "plt.xlabel(\"Polarity Sentiments\",fontsize=20)\n",
    "plt.ylabel(\"Frequency\",fontsize=20)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.title(\"Polarity Sentiments wrt Frequency\",fontsize= 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ca969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b0690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequent words for rating 1\n",
    "\n",
    "group_by = data.groupby('Rating')['Cleaned_Review_Lemmatized'].apply(lambda x: Counter(' '.join(x).split()).most_common(25))\n",
    "group_by_0 = group_by.iloc[0]\n",
    "words0 = list(zip(*group_by_0))[0]\n",
    "freq0 = list(zip(*group_by_0))[1]\n",
    "plt.figure(figsize=(50,30))\n",
    "plt.bar(words0, freq0,color=\"green\",edgecolor=\"orange\")\n",
    "plt.xlabel('Words', fontsize=50)\n",
    "plt.ylabel('Frequency of Words', fontsize=50)\n",
    "plt.yticks(fontsize=40)\n",
    "plt.xticks(rotation=60, fontsize=40)\n",
    "plt.title('Frequency of 25 Most Common Words for Rating=1', fontsize=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c4cdb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Most frequent words for rating 2\n",
    "\n",
    "group_by_1 = group_by.iloc[1]\n",
    "words1 = list(zip(*group_by_1))[0]\n",
    "freq1 = list(zip(*group_by_1))[1]\n",
    "plt.figure(figsize=(50,30))\n",
    "plt.bar(words1, freq1,color=\"green\",edgecolor=\"orange\")\n",
    "plt.xlabel('Words', fontsize=50)\n",
    "plt.ylabel('Frequency of Words', fontsize=50)\n",
    "plt.yticks(fontsize=40)\n",
    "plt.xticks(rotation=60, fontsize=40)\n",
    "plt.title('Frequency of 25 Most Common Words for Rating=2', fontsize=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81683961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequent words for rating 3\n",
    "\n",
    "group_by_2 = group_by.iloc[2]\n",
    "words2 = list(zip(*group_by_2))[0]\n",
    "freq2 = list(zip(*group_by_2))[1]\n",
    "plt.figure(figsize=(50,30))\n",
    "plt.bar(words2, freq2,color=\"green\",edgecolor=\"orange\")\n",
    "plt.xlabel('Words', fontsize=50)\n",
    "plt.ylabel('Frequency of Words', fontsize=50)\n",
    "plt.yticks(fontsize=40)\n",
    "plt.xticks(rotation=60, fontsize=40)\n",
    "plt.title('Frequency of 25 Most Common Words for Rating=3', fontsize=60)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bac9387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequent words for rating 4\n",
    "\n",
    "group_by_3 = group_by.iloc[3]\n",
    "words3 = list(zip(*group_by_3))[0]\n",
    "freq3 = list(zip(*group_by_3))[1]\n",
    "plt.figure(figsize=(50,30))\n",
    "plt.bar(words3, freq3,color=\"green\",edgecolor=\"orange\")\n",
    "plt.xlabel('Words', fontsize=50)\n",
    "plt.ylabel('Frequency of Words', fontsize=50)\n",
    "plt.yticks(fontsize=40)\n",
    "plt.xticks(rotation=60, fontsize=40)\n",
    "plt.title('Frequency of 25 Most Common Words for Rating=4', fontsize=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377adefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequent words for rating 5\n",
    "\n",
    "group_by_4 = group_by.iloc[4]\n",
    "words4 = list(zip(*group_by_4))[0]\n",
    "freq4 = list(zip(*group_by_4))[1]\n",
    "plt.figure(figsize=(50,30))\n",
    "plt.bar(words4, freq4,color=\"green\",edgecolor=\"orange\")\n",
    "plt.xlabel('Words', fontsize=50)\n",
    "plt.ylabel('Frequency of Words', fontsize=50)\n",
    "plt.yticks(fontsize=40)\n",
    "plt.xticks(rotation=60, fontsize=40)\n",
    "plt.title('Frequency of 25 Most Common Words for Rating=5', fontsize=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f57af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic modelling using LDA#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821cf2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b442a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf32cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy for lemmatization\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2316990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37442acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c memex pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f274ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term Frequency with Count Vetorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop word\n",
    "stop_words_keywords = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a85531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special additioanl stop words added for keyword extraction\n",
    "stop_words_keywords.extend([\n",
    "    \"will\", \"always\", \"go\", \"one\", \"very\", \"good\", \"only\", \"mr\", \"lot\", \"two\",\n",
    "    \"th\", \"etc\", \"don\", \"due\", \"didn\", \"since\", \"nt\", \"ms\", \"ok\", \"almost\",\n",
    "    \"put\", \"pm\", \"hyatt\", \"grand\", \"till\", \"add\", \"let\", \"hotel\", \"able\",\n",
    "    \"per\", \"st\", \"couldn\", \"yet\", \"par\", \"hi\", \"well\", \"would\", \"I\", \"the\",\n",
    "    \"s\", \"also\", \"great\", \"get\", \"like\", \"take\", \"thank\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae256728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most frequent words in the data, extracting information about its content and topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36c118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2f792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Afinn\n",
    "from afinn import Afinn\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a1c1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab733b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english') + ['com'])\n",
    "co = CountVectorizer(stop_words=stop_words_keywords)\n",
    "counts = co.fit_transform(corpus)\n",
    "most_freq_terms = pd.DataFrame(counts.sum(axis=0),\n",
    "                               columns=co.get_feature_names()).T.sort_values(\n",
    "                                   0, ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e042691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_freq_terms.plot(kind='bar', title='Unigram Frequency', figsize=(15, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba38f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can check for frequent bi-grams:\n",
    "co = CountVectorizer(ngram_range=(2, 2), stop_words=stop_words_keywords)\n",
    "counts1 = co.fit_transform(corpus)\n",
    "bi_grams = pd.DataFrame(counts1.sum(axis=0),\n",
    "                        columns=co.get_feature_names()).T.sort_values(\n",
    "                            0, ascending=False).head(25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31d28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_grams.plot(kind='bar', title='bi-grams', figsize=(13, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c926fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can check for frequent tri-grams:\n",
    "co = CountVectorizer(ngram_range=(3, 3), stop_words=stop_words_keywords)\n",
    "counts1 = co.fit_transform(corpus)\n",
    "tri_grams = pd.DataFrame(counts1.sum(axis=0),\n",
    "                         columns=co.get_feature_names()).T.sort_values(\n",
    "                             0, ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb08840",
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_grams.plot(kind='bar', title='tri-grams', figsize=(13, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a85a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term Frequency with TFIDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2dbc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting text into vectors by TFIDF\n",
    "tfidf2 = TfidfVectorizer(norm=\"l2\",analyzer='word', stop_words=stop_words_keywords,ngram_range=(1,1))\n",
    "tfidf2_x = tfidf2.fit_transform(corpus)\n",
    "most_freq_terms = pd.DataFrame(tfidf2_x.sum(axis=0),\n",
    "                               columns=tfidf2.get_feature_names()).T.sort_values(\n",
    "                                   0, ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92286f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_freq_terms.plot(kind='bar',\n",
    "                     title='most frequent terms & their frequency',\n",
    "                     figsize=(15, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff7345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can check for frequent bi-grams:\n",
    "\n",
    "tfidf2 = TfidfVectorizer(norm=\"l2\",\n",
    "                         analyzer='word',\n",
    "                         stop_words=stop_words_keywords,\n",
    "                         ngram_range=(2, 2))\n",
    "tfidf2_x = tfidf2.fit_transform(corpus)\n",
    "most_freq_terms = pd.DataFrame(\n",
    "    tfidf2_x.sum(axis=0),\n",
    "    columns=tfidf2.get_feature_names()).T.sort_values(0,\n",
    "                                                      ascending=False).head(25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad3b791",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 22})\n",
    "most_freq_terms.plot(kind='bar', title='Bi-Gram', figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc3dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can check for frequent tri-grams:\n",
    "\n",
    "tfidf2 = TfidfVectorizer(norm=\"l2\",\n",
    "                         analyzer='word',\n",
    "                         stop_words=stop_words_keywords,\n",
    "                         ngram_range=(3, 3))\n",
    "tfidf2_x = tfidf2.fit_transform(corpus)\n",
    "most_freq_terms = pd.DataFrame(\n",
    "    tfidf2_x.sum(axis=0),\n",
    "    columns=tfidf2.get_feature_names()).T.sort_values(0,\n",
    "                                                      ascending=False).head(25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 22})\n",
    "most_freq_terms.plot(kind='bar', title='Tri-Gram', figsize=(15, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1000c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important Attribute Extraction from the Reviews\n",
    "CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de3d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = CountVectorizer(stop_words=stop_words_keywords,ngram_range=(1,2))\n",
    "cv2_x = cv2.fit_transform(corpus)\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf_transformer.fit(cv2_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc22f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature names\n",
    "feature_names = cv2.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08ce997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch document for which keywords needs to be extracted\n",
    "doc = corpus[532]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e5d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate tf-idf for the given document\n",
    "tf_idf_vector = tfidf_transformer.transform(cv2.transform([doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd32641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for sorting tf_idf in descending order\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebc8a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e3d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacce317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature extraction by converting text in vector by using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5561966",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=3000,\n",
    "                       ngram_range=(1,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647b424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF for X-train\n",
    "corpus_tfidf= tfidf.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a2cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save & load models\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb5b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'model_TFIDF.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50350eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tfidf, open('model_TFIDF.sav', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbce0ad1",
   "metadata": {},
   "source": [
    "####    X and Y Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "afinn=Afinn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11844f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affin Sentiment Score\n",
    "score = [afn.score(item) for item in data['Cleaned_Reviews']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e09a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive review = 1\n",
    "# Negative review = 0\n",
    "\n",
    "# Affin Sentiment Making it a two class classification problem\n",
    "Affin_sentiment = [1 if score > 0 else 0 for score in score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec2e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affin Sentiment & Score added to the data frame\n",
    "data['Affin_score'] = score\n",
    "data['Affin_sentiment'] = Affin_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccb9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing only the negative reviews\n",
    "data[data['Affin_sentiment'] == 0]['Cleaned_Reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83f720a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb5f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of X and Y variable\n",
    "X = corpus_tfidf\n",
    "y = data['Affin_sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7434cb",
   "metadata": {},
   "source": [
    "#Handling imbalanced data\n",
    "We will have to handling the imbalanced data set by using SMOTE Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb57847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of the imbalanced Y variable\n",
    "sns.countplot(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c41b9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the minority class will over sampled by 80%\n",
    "stregegy = 0.8\n",
    "\n",
    "# initializing of SMOTEN Sampling\n",
    "over = SMOTE(sampling_strategy= stregegy,random_state=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039a0a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over Samplled X and Y variable \n",
    "X_over, y_over = over.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5edbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y_over)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1445e7d",
   "metadata": {},
   "source": [
    "Split the Data into Train and Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600f88d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Validation\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ab1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_over, y_over,random_state=7,test_size=0.20,stratify=y_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061699e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train = {}\".format(X_train.shape))\n",
    "print(\"X_test = {}\".format(X_test.shape))\n",
    "print(\"y_train = {}\".format(y_train.shape))\n",
    "print(\"y_test = {}\".format(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c70c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e3fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99b7350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Accuracy\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa356e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom Fuction to run the model with confusion matrix + Train and test accuracy + F1score\n",
    "def model_metric(X_train, X_test, y_train, y_test, model, name):\n",
    "\n",
    "    # predicted for X_train and X_test\n",
    "    predict_train = model.predict(X_train)\n",
    "    predict_test = model.predict(X_test)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy_train = model.score(X_train, y_train)\n",
    "    accuracy_test = model.score(X_test, y_test)\n",
    "\n",
    "    # F1score\n",
    "    f1_score = metrics.f1_score(y_train, predict_train)\n",
    "\n",
    "    print('Accuracy for Train set {}'.format(accuracy_train))\n",
    "    print('Accuracy for Test set {}'.format(accuracy_test))\n",
    "    print('Fi Score {}'.format(f1_score))\n",
    "    \n",
    "    print(classification_report(y_test, predict_test))\n",
    "\n",
    "    plot_confusion_matrix(estimator=model,\n",
    "                          X=X_test,\n",
    "                          y_true=y_test,\n",
    "                          cmap='Blues')\n",
    "    plt.grid(False)\n",
    "    plt.title('{} - Confusion Matrix on Test set'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5e412",
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f3129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression and fit the model\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f92d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict for X dataset\n",
    "y_pred_train = classifier.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05936481",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_df = pd.DataFrame({'actual': y_train,\n",
    "                         'predicted_prob': y_pred_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e415cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f40e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f'Training-set accuracy score: {round(accuracy_score(y_train, y_pred_train),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21094694",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fbfc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_df= pd.DataFrame({'actual': y_test,\n",
    "                         'predicted_prob': y_pred_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8bcc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc8b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_val = accuracy_score(y_test, y_pred_test)\n",
    "acc_val_rounded = round(acc_val,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e522e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Testing-set accuracy score: {acc_val_rounded}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922fe37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save & load models\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a036b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d7cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a72a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b81a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97691d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e41c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
